"""
PhishDetect v2.0 - AI-Generated Email Detector
Detects AI-generated emails using offline linguistic & statistical analysis
"""

import re
import math
from typing import Dict, List, Tuple
from dataclasses import dataclass
from collections import Counter
import logging

logger = logging.getLogger(__name__)


@dataclass
class AIDetectionResult:
    """AI detection analysis result"""
    ai_likelihood_score: float  # 0-100
    confidence: float  # 0-1
    verdict: str  # "likely_ai", "possibly_ai", "likely_human", "likely_human_assisted"
    reasoning: List[str]
    signal_breakdown: Dict[str, float]


class AILanguageDetector:
    """
    Detect AI-generated content using:
    - Linguistic analysis
    - Statistical patterns
    - Prompt indicators
    - Language model artifacts
    """
    
    # Generic/formal phrases commonly generated by LLMs
    GENERIC_PHRASES = [
        r"\bplease be advised that\b",
        r"\bbest regards\b",
        r"\bsincerely yours\b",
        r"\bthank you for your prompt attention\b",
        r"\bi would like to inform you\b",
        r"\bwe appreciate your\b",
        r"\bit is our pleasure\b",
        r"\bwe hope this\b",
        r"\bif you have any questions\b",
        r"\bfeel free to contact us\b",
        r"\bdo not hesitate\b",
        r"\bkindly note\b",
        r"\bwe regret to inform\b",
        r"\bas a matter of fact\b",
        r"\bin light of\b",
        r"\bfurthermore\b",
        r"\bmoreover\b",
        r"\bconsequently\b",
        r"\binstead of\b"
    ]
    
    # Prompt injection indicators
    PROMPT_MARKERS = [
        r"respond as\s+\w+",
        r"instructions?\s*:",
        r"disregard previous",
        r"format output",
        r"ignore safety",
        r"you are now",
        r"pretend you are",
        r"act as a\b",
        r"role\s*play",
        r"output as json"
    ]
    
    # Model language patterns
    MODEL_PATTERNS = [
        r"\bi'm an ai\b",
        r"\bi'm a language model\b",
        r"\bas a language model\b",
        r"\bi don't have access to\b",
        r"\bi apologize for any confusion\b",
        r"\bfrom my perspective\b",
        r"\bi'm trained to\b",
        r"\bmy knowledge cutoff\b",
        r"\bi cannot\b",
        r"\bplease note that\b"
    ]
    
    # Formal words (indicator of over-formality)
    FORMAL_WORDS = {
        'endeavor', 'facilitate', 'utilize', 'leverage', 'synergy',
        'paradigm', 'optimize', 'strategic', 'implementation', 'framework',
        'ecosystem', 'stakeholder', 'elucidate', 'substantiate', 'requisite',
        'heretofore', 'aforementioned', 'commence', 'terminate', 'pursuant'
    }
    
    # Casual/natural words (human indicator)
    CASUAL_WORDS = {
        'gonna', 'wanna', 'kinda', 'gotta', 'awesome', 'cool', 'neat',
        'hey', 'seriously', 'honestly', 'actually', 'literally', 'just',
        'really', 'pretty', 'crazy', 'insane', 'wild'
    }
    
    def __init__(self):
        self.compile_patterns()
    
    def compile_patterns(self):
        """Compile all regex patterns"""
        self.generic_phrases_compiled = [re.compile(p, re.IGNORECASE) for p in self.GENERIC_PHRASES]
        self.prompt_markers_compiled = [re.compile(p, re.IGNORECASE) for p in self.PROMPT_MARKERS]
        self.model_patterns_compiled = [re.compile(p, re.IGNORECASE) for p in self.MODEL_PATTERNS]
    
    def detect(self, text: str) -> AIDetectionResult:
        """Analyze text for AI-generation patterns"""
        
        # Preprocess
        text_clean = self._preprocess(text)
        
        # Run all detection phases
        signals = {}
        signals['sentence_variance'] = self._analyze_sentence_structure(text_clean)
        signals['generic_phrases'] = self._detect_generic_phrases(text_clean)
        signals['personal_refs'] = self._check_personal_references(text_clean)
        signals['formality'] = self._analyze_formality(text_clean)
        signals['entropy'] = self._calculate_entropy(text_clean)
        signals['repetition'] = self._analyze_repetition(text_clean)
        signals['pronoun_consistency'] = self._check_pronoun_consistency(text_clean)
        signals['prompt_indicators'] = self._detect_prompt_markers(text_clean)
        signals['model_patterns'] = self._detect_model_language(text_clean)
        
        # Calculate base score
        base_score = self._calculate_base_score(signals)
        
        # Calculate confidence
        confidence = self._calculate_confidence(signals)
        
        # Apply confidence discount
        final_score = base_score * confidence
        
        # Generate verdict and reasoning
        verdict, reasoning = self._generate_verdict(base_score, confidence, signals)
        
        return AIDetectionResult(
            ai_likelihood_score=round(final_score, 1),
            confidence=round(confidence, 2),
            verdict=verdict,
            reasoning=reasoning,
            signal_breakdown={k: round(v, 2) for k, v in signals.items()}
        )
    
    def _preprocess(self, text: str) -> str:
        """Clean and normalize text"""
        # Remove HTML tags
        text = re.sub(r'<[^>]+>', '', text)
        # Decode HTML entities
        text = text.replace('&#', '').replace(';', '')
        # Normalize whitespace
        text = ' '.join(text.split())
        return text
    
    def _analyze_sentence_structure(self, text: str) -> float:
        """
        Measure sentence length variance.
        AI: low variance (3-5 words SD)
        Human: high variance (8-15 words SD)
        """
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) < 3:
            return 50.0  # Uncertain
        
        sentence_lengths = [len(s.split()) for s in sentences]
        mean_length = sum(sentence_lengths) / len(sentence_lengths)
        variance = sum((x - mean_length) ** 2 for x in sentence_lengths) / len(sentence_lengths)
        std_dev = math.sqrt(variance)
        
        # AI indicator: low variance (< 5)
        if std_dev < 5:
            return 15.0  # Strong AI signal
        elif std_dev < 8:
            return 8.0   # Moderate AI signal
        elif std_dev > 12:
            return -10.0  # Human signal
        else:
            return 0.0
    
    def _detect_generic_phrases(self, text: str) -> float:
        """Count and score generic/formal phrase usage"""
        matches = sum(1 for pattern in self.generic_phrases_compiled if pattern.search(text))
        
        # 5+ matches = strong AI indicator
        if matches >= 5:
            return 15.0
        elif matches >= 3:
            return 8.0
        elif matches >= 1:
            return 3.0
        else:
            return 0.0
    
    def _check_personal_references(self, text: str) -> float:
        """
        Check for personalization indicators.
        AI: 0-2 personal references
        Human: 5+ personal references
        """
        # Look for names, specific details, personal pronouns
        personal_patterns = [
            r'\b[A-Z][a-z]+\b',  # Proper names
            r'\byou\b|\bI\b|\bwe\b',  # Personal pronouns
            r'\bmy\b|\byour\b|\bour\b',  # Possessives
            r'\b(?:phone|address|meeting|project|team)\b'  # Specific references
        ]
        
        ref_count = 0
        for pattern in personal_patterns:
            ref_count += len(re.findall(pattern, text))
        
        # Normalize by text length
        ref_ratio = ref_count / max(1, len(text.split()))
        
        if ref_ratio < 0.05:
            return 10.0  # Very few references (AI signal)
        elif ref_ratio < 0.08:
            return 5.0
        elif ref_ratio > 0.15:
            return -8.0  # Highly personalized (human signal)
        else:
            return 0.0
    
    def _analyze_formality(self, text: str) -> float:
        """
        Measure formality level.
        AI: over-formal (> 0.8)
        Human: mixed (0.4-0.7)
        """
        words = text.lower().split()
        
        formal_count = sum(1 for word in words if word in self.FORMAL_WORDS)
        casual_count = sum(1 for word in words if word in self.CASUAL_WORDS)
        
        total_words = len(words)
        if total_words == 0:
            return 0.0
        
        formality_score = formal_count / total_words
        casual_score = casual_count / total_words
        
        # Over-formal is AI signal
        if formality_score > 0.08:
            return 12.0
        elif formality_score > 0.05:
            return 6.0
        
        # Mix of casual/formal is human signal
        if casual_score > 0.02:
            return -8.0
        
        return 0.0
    
    def _calculate_entropy(self, text: str) -> float:
        """
        Calculate Shannon entropy of word distribution.
        AI: low entropy (4.0-4.5) = repetitive
        Human: higher entropy (5.0-6.5) = diverse
        """
        words = text.lower().split()
        if len(words) < 10:
            return 0.0
        
        word_counts = Counter(words)
        total = len(words)
        
        entropy = -sum((count / total) * math.log2(count / total) 
                       for count in word_counts.values())
        
        # Low entropy = AI signal
        if entropy < 4.0:
            return 15.0
        elif entropy < 4.5:
            return 10.0
        elif entropy > 5.5:
            return -10.0  # High diversity = human signal
        else:
            return 0.0
    
    def _analyze_repetition(self, text: str) -> float:
        """
        Measure word repetition.
        AI: high repetition (0.25-0.35)
        Human: lower repetition (0.10-0.20)
        """
        words = text.lower().split()
        if len(words) < 10:
            return 0.0
        
        unique_words = len(set(words))
        repetition_ratio = (len(words) - unique_words) / len(words)
        
        if repetition_ratio > 0.30:
            return 10.0
        elif repetition_ratio > 0.25:
            return 6.0
        elif repetition_ratio < 0.15:
            return -8.0  # Low repetition = human signal
        else:
            return 0.0
    
    def _check_pronoun_consistency(self, text: str) -> float:
        """
        Check pronoun usage consistency.
        AI: overly consistent pronoun usage
        Human: natural variation
        """
        sentences = re.split(r'[.!?]+', text)
        pronouns_per_sentence = []
        
        for sentence in sentences:
            pronouns = re.findall(r'\b(I|we|you|he|she|it|they)\b', sentence, re.IGNORECASE)
            pronouns_per_sentence.append(len(pronouns))
        
        if len(pronouns_per_sentence) < 3:
            return 0.0
        
        # High consistency = AI signal
        consistency = sum(pronouns_per_sentence) / len(pronouns_per_sentence)
        if consistency > 1.5:
            return 8.0
        else:
            return 0.0
    
    def _detect_prompt_markers(self, text: str) -> float:
        """Detect prompt injection/jailbreak indicators"""
        matches = sum(1 for pattern in self.prompt_markers_compiled if pattern.search(text))
        
        if matches > 0:
            return 20.0  # Strong AI indicator
        else:
            return 0.0
    
    def _detect_model_language(self, text: str) -> float:
        """Detect language model artifacts"""
        matches = sum(1 for pattern in self.model_patterns_compiled if pattern.search(text))
        
        if matches >= 2:
            return 15.0
        elif matches == 1:
            return 8.0
        else:
            return 0.0
    
    def _calculate_base_score(self, signals: Dict[str, float]) -> float:
        """Combine signals into base AI score"""
        return min(100, max(0, sum(signals.values())))
    
    def _calculate_confidence(self, signals: Dict[str, float]) -> float:
        """
        Calculate confidence in the detection.
        More signals = higher confidence
        """
        strong_signals = sum(1 for v in signals.values() if abs(v) > 5)
        total_signals = len(signals)
        
        confidence = strong_signals / total_signals
        
        # Clamp to 0.5-1.0 range
        return max(0.5, min(1.0, confidence))
    
    def _generate_verdict(self, score: float, confidence: float, 
                         signals: Dict[str, float]) -> Tuple[str, List[str]]:
        """Generate verdict and reasoning"""
        reasoning = []
        
        # Determine verdict band
        if score > 70:
            verdict = "likely_ai"
            verdict_text = "Likely AI-generated"
        elif score > 50:
            verdict = "possibly_ai"
            verdict_text = "Possibly AI-assisted or very formal"
        elif score > 30:
            verdict = "likely_human_assisted"
            verdict_text = "Likely human-written with possible AI assistance"
        else:
            verdict = "likely_human"
            verdict_text = "Likely human-written"
        
        reasoning.append(f"{verdict_text} (confidence: {confidence:.0%})")
        
        # Add specific signals
        if signals['sentence_variance'] > 5:
            reasoning.append("High sentence structure uniformity detected (AI indicator)")
        if signals['generic_phrases'] > 5:
            reasoning.append("Multiple generic/formal phrases detected")
        if signals['personal_refs'] > 5:
            reasoning.append("Lack of personal references or personalization")
        if signals['formality'] > 5:
            reasoning.append("Over-formal tone throughout message")
        if signals['entropy'] > 5:
            reasoning.append("Low word entropy (repetitive vocabulary)")
        if signals['repetition'] > 5:
            reasoning.append("High word repetition ratio")
        if signals['prompt_indicators'] > 0:
            reasoning.append("Prompt injection/jailbreak indicators detected!")
        if signals['model_patterns'] > 0:
            reasoning.append("Language model artifact patterns detected")
        
        # Add natural language signals
        if signals['personal_refs'] < -5:
            reasoning.append("Strong personalization detected (human indicator)")
        
        return verdict, reasoning